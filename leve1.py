from telethon import TelegramClient
from telethon.sessions import StringSession
from telethon.errors import FloodWaitError
from dotenv import load_dotenv
import asyncio
import os
import time

# ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
load_dotenv()

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…
API_ID = int(os.getenv('API_ID', 0))
API_HASH = os.getenv('API_HASH')
SESSION = os.getenv('SESSION')  
CHANNEL_ID = int(os.getenv('CHANNEL_ID', 0))  
CHANNEL_ID_LOG = int(os.getenv('CHANNEL_ID_LOG', 0))  
FIRST_MSG_ID = int(os.getenv('FIRST_MSG_ID', 0))  

# Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª ÙˆØ£Ø¯Ø§Ø¡
total_deleted_count = 0
total_saved_space = 0  # Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø­Ø±Ø±Ø©
processing_times = []  # Ù„ØªØªØ¨Ø¹ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‡Ø§Ù…
start_time = None  # ÙˆÙ‚Øª Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„

async def collect_files(client, channel_id, first_msg_id):
    """Ø¥ØµØ¯Ø§Ø± Ù…Ø­Ø³Ù† Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡"""
    global processing_times
    file_dict = {}
    
    start_collect = time.time()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© ØºÙŠØ± Ù…ØªØ²Ø§Ù…Ù†Ø© Ø¨Ù…Ù‡Ø§Ù… Ù…Ø¬Ù…Ø¹Ø©
    async def process_message(message):
        if message.file and hasattr(message.file, 'size'):
            file_size = message.file.size
            async with lock:  # Ù…Ù†Ø¹ Ø§Ù„ØªÙ†Ø§ÙØ³ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
                if file_size in file_dict:
                    file_dict[file_size].append((message.id, file_size))
                else:
                    file_dict[file_size] = [(message.id, file_size)]
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‡Ø§Ù… Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
    tasks = []
    lock = asyncio.Lock()
    async for message in client.iter_messages(channel_id, min_id=first_msg_id):
        tasks.append(process_message(message))
        if len(tasks) % 100 == 0:  # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ 100 Ø±Ø³Ø§Ù„Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
            await asyncio.gather(*tasks)
            tasks = []
    
    if tasks:  # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
        await asyncio.gather(*tasks)
    
    processing_time = time.time() - start_collect
    processing_times.append(('collect_files', processing_time))
    return file_dict

async def forward_delete_and_send_original_link(client, source_chat, destination_chat, duplicate_msg_ids):
    """Ø¥ØµØ¯Ø§Ø± Ù…Ø­Ø³Ù† Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
    global total_deleted_count, total_saved_space
    chunk_size = 99
    
    original_msg_id, original_size = duplicate_msg_ids[0]
    duplicates = duplicate_msg_ids[1:]
    
    total_saved_space += original_size * len(duplicates)  # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø­Ø±Ø±Ø©
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯ÙØ¹Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
    tasks = []
    for i in range(0, len(duplicates), chunk_size):
        chunk = duplicates[i:i + chunk_size]
        tasks.append(process_chunk(client, source_chat, destination_chat, chunk, original_msg_id))
    
    await asyncio.gather(*tasks)

async def process_chunk(client, source_chat, dest_chat, chunk, original_id):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ù…Ø¹ ØªØªØ¨Ø¹ Ø§Ù„ÙˆÙ‚Øª"""
    global total_deleted_count
    start_chunk = time.time()
    
    try:
        # Ù†Ù‚Ù„ ÙˆØ­Ø°Ù Ù…ØªÙˆØ§Ø²ÙŠ
        await asyncio.gather(
            client.forward_messages(dest_chat, [msg_id for msg_id, _ in chunk], from_peer=source_chat),
            client.delete_messages(source_chat, [msg_id for msg_id, _ in chunk])
        )
        
        total_deleted_count += len(chunk)
        print(f"âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {len(chunk)} Ø±Ø³Ø§Ù„Ø©")
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ
        original_link = f"https://t.me/c/{str(source_chat)[4:]}/{original_id}"
        await client.send_message(dest_chat, f"ğŸ“Œ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_link}")
        
    except FloodWaitError as e:
        print(f"â³ Ø§Ù†ØªØ¸Ø± {e.seconds} Ø«Ø§Ù†ÙŠØ©")
        await asyncio.sleep(e.seconds + 1)
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø£: {e}")
    
    processing_times.append(('process_chunk', time.time() - start_chunk))

async def send_statistics(client):
    """Ø¥Ø±Ø³Ø§Ù„ ØªÙ‚Ø±ÙŠØ± Ø¥Ø­ØµØ§Ø¦ÙŠ Ù…ÙØµÙ„"""
    global total_deleted_count, total_saved_space, start_time
    
    total_time = time.time() - start_time
    avg_time = sum(t[1] for t in processing_times) / len(processing_times) if processing_times else 0
    
    report = f"""
    ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡ ğŸ“Š
    --------------------
    â€¢ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {total_deleted_count} ğŸ—‘
    â€¢ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø­Ø±Ø±Ø©: {total_saved_space/1024/1024:.2f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª ğŸ’¾
    â€¢ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙƒÙ„ÙŠ: {total_time:.2f} Ø«Ø§Ù†ÙŠØ© â±
    â€¢ Ù…ØªÙˆØ³Ø· ÙˆÙ‚Øª Ø§Ù„Ù…Ù‡Ù…Ø©: {avg_time:.2f} Ø«Ø§Ù†ÙŠØ© âš¡
    â€¢ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø£Ø¨Ø·Ø£: 
    {sorted(processing_times, key=lambda x: x[1], reverse=True)[:3]}
    """
    
    await client.send_message(CHANNEL_ID_LOG, report)
    print("âœ… ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠ")

async def delete_duplicates(client, channel_id):
    global start_time
    start_time = time.time()
    
    print("ğŸ” Ø¨Ø¯Ø£ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª...")
    file_dict = await collect_files(client, channel_id, FIRST_MSG_ID)
    
    print("âš¡ Ø¨Ø¯Ø£ Ø­Ø°Ù Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª...")
    tasks = []
    for file_size, msg_list in file_dict.items():
        if len(msg_list) > 1:
            tasks.append(forward_delete_and_send_original_link(
                client, channel_id, CHANNEL_ID_LOG, msg_list))
    
    await asyncio.gather(*tasks)
    
    await send_statistics(client)
    print(f"ğŸ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙÙŠ {time.time()-start_time:.2f} Ø«Ø§Ù†ÙŠØ©")

async def main():
    async with TelegramClient(StringSession(SESSION), API_ID, API_HASH) as client:
        print("ğŸš€ Ø§ØªØµØ§Ù„ Ù†Ø§Ø¬Ø­ Ø¨Ø§Ù„ØªÙŠÙ„ÙŠØ¬Ø±Ø§Ù…")
        await delete_duplicates(client, CHANNEL_ID)

if __name__ == '__main__':
    print("ğŸ”¹ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„...")
    asyncio.run(main())
